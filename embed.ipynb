{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Vector DB\n",
    "\n",
    "`conda activate env_ragstack`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Web Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a number of HTML pages using `request` module. Each of those pages contains lots of superfluous content so we extract only the relevant article context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\"\"\"\n",
    "Function to clean text from web pages\n",
    "\"\"\"\n",
    "def clean_text(text: str):\n",
    "    # Normalize line breaks to \\n\\n (two new lines)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\r\", \"\\n\\n\")\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Remove leading spaces before removing trailing spaces\n",
    "    text = re.sub(\"^[ \\t]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove trailing spaces before removing empty lines\n",
    "    text = re.sub(\"[ \\t]+$\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty lines\n",
    "    text = re.sub(\"^\\s+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove unicode Non Breaking Space\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the number of tokens in a text string.\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Website to TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def website_to_txt(job_url: str):\n",
    "\n",
    "    item = {}\n",
    "    item_title = \"\"\n",
    "    item_price = \"\"\n",
    "    item_description = \"\"\n",
    "    product_description = \"\"\n",
    "    product_details = \"\"\n",
    "\n",
    "    try:\n",
    "        page = requests.get(job_url)\n",
    "\n",
    "        if page.status_code != 200:\n",
    "            print(f\"Failed to retrieve the job posting at {job_url}. Status code: {page.status_code}\")\n",
    "\n",
    "        # Parse the HTML content of the job posting using BeautifulSoup\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Find the page title element and get the text\n",
    "        item_title = soup.find('h1', {'class': 'product__title'})\n",
    "        if item_title is not None:\n",
    "            item_title = item_title.text.strip()\n",
    "        else:\n",
    "            item_title = \"\"\n",
    "\n",
    "\n",
    "        # Find the item price\n",
    "        item_price = soup.find('div', {'class': 'product__price'})\n",
    "        if item_price is not None:\n",
    "            item_price = item_price.find_all('span')\n",
    "            item_price = [item_price.text for item_price in item_price]\n",
    "            item_price = item_price[0]\n",
    "            #strip $ fro the price\n",
    "            item_price = item_price[1:]\n",
    "        else:\n",
    "            item_price = \"\"\n",
    "\n",
    "\n",
    "        # Find the page description element\n",
    "        item_description = soup.find('div', {'class': 'product__accordion'})\n",
    "        if item_description is not None:\n",
    "\n",
    "            # Product Description\n",
    "            product_description = item_description.find('div', {'id': 'accordion-panel-1'})\n",
    "            if product_description is not None:\n",
    "                product_description = product_description.text.strip()\n",
    "            else: \n",
    "                product_description = \"\"\n",
    "\n",
    "            # Product Details\n",
    "            product_details = item_description.find('div', {'id': 'accordion-panel-2'})\n",
    "            if product_details is not None:\n",
    "                # from the unordered list, get the text of each list item\n",
    "                product_details = product_details.find_all('li')\n",
    "                product_details = [li.text for li in product_details]\n",
    "                product_details = \"\\n\".join(product_details)\n",
    "            else: \n",
    "                product_details = \"\"        \n",
    "            \n",
    "        else:\n",
    "            item_description = \"\"\n",
    "    \n",
    "        # Find the item image\n",
    "        item_img = soup.find('div', {'class': 'product__media'})\n",
    "        if item_img is not None:\n",
    "            item_img = item_img.find('img', {'class': 'component-image__image'})\n",
    "            item_img = item_img['src']\n",
    "            item_img = item_img.split('?')[0]\n",
    "        else:\n",
    "            item_img = \"\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not get the description from the URL: {job_url}\")\n",
    "        logging.error(e)\n",
    "        exit()\n",
    "\n",
    "    item['title'] = item_title\n",
    "    item['price'] = item_price\n",
    "    item['img'] = item_img\n",
    "    item['description'] = f\"Description:\\n{clean_text(product_description)}\\n\\nDetails:\\n{product_details}\"\n",
    "\n",
    "\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get URLs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of URLs: 227\n"
     ]
    }
   ],
   "source": [
    "#read urls from file named links.txt\n",
    "with open('links2.txt', 'r') as file:\n",
    "    urls = file.readlines()\n",
    "    urls = [url.strip() for url in urls]\n",
    "\n",
    "# prepend \"https://blueillusion.com/\" to each url\n",
    "urls = [\"https://blueillusion.com\" + url for url in urls]\n",
    "\n",
    "# For debugging, use only a single URL\n",
    "#urls = [\"https://blueillusion.com/products/waist-tab-linen-culotte-216716lnm-chambray-cross-dye\"]\n",
    "\n",
    "print (f\"Number of URLs: {len(urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Documents: 227\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "data = []\n",
    "for url in urls:\n",
    "    item = website_to_txt(url)\n",
    "    \n",
    "    metadata = {\n",
    "        'source': url,\n",
    "        'title': item['title'],\n",
    "        'price': item['price'],\n",
    "        'img': item['img'],\n",
    "        'language': 'en'\n",
    "    }\n",
    "\n",
    "    document = Document(page_content=item['description'], metadata=metadata)\n",
    "\n",
    "    data.append(document)\n",
    "\n",
    "print (f\"Number of Documents: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Description:\\nIntroducing ‘The Trapeze’ from French eyewear specialists IZIPIZI. These comfortable, lightweight frames are perfect for navigating the warm weather. Expertly crafted in a lightweight rubber texture, these sunglasses offer 100% UV protection. With a large, structured trapeze silhouette that is flattering to many, these sunglasses also showcase a bright pink colour that makes for a modern aesthetic. Take these sunglasses with you on your next outing or holiday.Style/SKU: A19230.493\\n\\nDetails:\\nIncludes storage pouch\\nRubber texture\\nTrapeze silhouette\\nFlexible arms\\n100% UV protection\\nPink' metadata={'source': 'https://blueillusion.com/products/trapeze-sunglasses-pink-a19230-pink', 'title': 'Trapeze Sunglasses Pink', 'price': '40.00', 'img': '//blueillusion.com/cdn/shop/files/trapezesunglassespink00006.jpg', 'language': 'en'}\n"
     ]
    }
   ],
   "source": [
    "print(data[199])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Crawled Data to Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#write data to file in a way that it can be reconstituted into a list of documents\n",
    "with open(\"help_data2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Crawled Data from Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read help_data.pkl and recreate data object as list of documents\n",
    "import pickle\n",
    "with open(\"help_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(len(data))\n",
    "print(data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting Data\n",
      "Number of chunks: 227\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# Chunk the data\n",
    "print(\"Splitting Data\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AstraDB Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "ASTRA_VECTOR_ENDPOINT = os.environ[\"ASTRA_VECTOR_ENDPOINT_BO\"]\n",
    "ASTRA_DB_KEYSPACE = \"blueillusion\"\n",
    "ASTRA_DB_COLLECTION = \"catalogue\"\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup Vector Store: blueillusion - catalogue\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.astradb import AstraDB\n",
    "\n",
    "#from langchain_community.embeddings import CohereEmbeddings\n",
    "#embeddings = CohereEmbeddings(model=\"embed-english-v3.0\")\n",
    "\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "#embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "embeddings = BedrockEmbeddings(credentials_profile_name=\"fieldops\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set up the vector store\n",
    "print(f\"Setup Vector Store: {ASTRA_DB_KEYSPACE} - {ASTRA_DB_COLLECTION}\")\n",
    "vectorstore = AstraDB(\n",
    "    embedding=embeddings,\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    collection_name=ASTRA_DB_COLLECTION,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_VECTOR_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store data and embeddings in Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding texts to Vector Store\n",
      "Adding 0 to 50 Inserted 50 documents.\n",
      "Adding 50 to 100 Inserted 50 documents.\n",
      "Adding 100 to 150 Inserted 50 documents.\n",
      "Adding 150 to 200 Inserted 50 documents.\n",
      "Adding 200 to 250 Inserted 27 documents.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Adding texts to Vector Store\")\n",
    "\n",
    "BLOCK_SIZE = 50\n",
    "# iterate through docs in sets of BLOCK_SIZE\n",
    "for i in range(0, len(docs), BLOCK_SIZE):\n",
    "    print(f\"Adding {i} to {i+BLOCK_SIZE}\", end=' ')\n",
    "    texts, metadatas = zip(*((doc.page_content, doc.metadata) for doc in docs[i:i+BLOCK_SIZE]))\n",
    "    inserted_ids = vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    print(f\"Inserted {len(inserted_ids)} documents.\")\n",
    "    # pause for 1 seconds\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all Documents in Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy.db import AstraDB\n",
    "\n",
    "# Initialize the AstraDB client\n",
    "db = AstraDB(\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_VECTOR_ENDPOINT,\n",
    ")\n",
    "\n",
    "# Retrieve collections\n",
    "collections_response = db.get_collections()\n",
    "\n",
    "# validate that ASTRA_DB_COLLECTION exists in collections_response[\"status\"][\"collections\"]\n",
    "if ASTRA_DB_COLLECTION in collections_response[\"status\"][\"collections\"]:\n",
    "    print(f\"Collection \\\"{ASTRA_DB_COLLECTION}\\\" exists\")\n",
    "\n",
    "    # Access an existing collection\n",
    "    collection = db.collection(ASTRA_DB_COLLECTION)\n",
    "\n",
    "    # Delete all documents in the collection\n",
    "    res = collection.delete_many(filter={})\n",
    "\n",
    "    # Print the result\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
