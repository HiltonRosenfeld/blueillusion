{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Data from Web Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a number of HTML pages using `request` module. Each of those pages contains lots of superfluous content so we extract only the relevant article context.\n",
    "\n",
    "The parsed data is saved into a Pickle file so that we don't have to crawl the website again if we need to recreate the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "\"\"\"\n",
    "Function to clean text from web pages\n",
    "\"\"\"\n",
    "def clean_text(text: str):\n",
    "    # Normalize line breaks to \\n\\n (two new lines)\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\\n\")\n",
    "    text = text.replace(\"\\r\", \"\\n\\n\")\n",
    "\n",
    "    # Replace two or more spaces with a single space\n",
    "    text = re.sub(\" {2,}\", \" \", text)\n",
    "\n",
    "    # Remove leading spaces before removing trailing spaces\n",
    "    text = re.sub(\"^[ \\t]+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove trailing spaces before removing empty lines\n",
    "    text = re.sub(\"[ \\t]+$\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove empty lines\n",
    "    text = re.sub(\"^\\s+\", \"\", text, flags=re.MULTILINE)\n",
    "\n",
    "    # remove unicode Non Breaking Space\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\"\"\"\n",
    "Function to calculate the number of tokens in a text string.\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Website to TXT function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads a URL and parses the content using Beautiful Soup. The parsing is specific to the Blue Illusion website and based on the CSS of the specific elements we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def website_to_txt(job_url: str):\n",
    "\n",
    "    item = {}\n",
    "    item_title = \"\"\n",
    "    item_price = \"\"\n",
    "    item_description = \"\"\n",
    "    product_description = \"\"\n",
    "    product_details = \"\"\n",
    "\n",
    "    try:\n",
    "        page = requests.get(job_url)\n",
    "\n",
    "        if page.status_code != 200:\n",
    "            print(f\"Failed to retrieve the job posting at {job_url}. Status code: {page.status_code}\")\n",
    "\n",
    "        # Parse the HTML content of the job posting using BeautifulSoup\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Find the page title element and get the text\n",
    "        item_title = soup.find('h1', {'class': 'product__title'})\n",
    "        if item_title is not None:\n",
    "            item_title = item_title.text.strip()\n",
    "        else:\n",
    "            item_title = \"\"\n",
    "\n",
    "\n",
    "        # Find the item price\n",
    "        item_price = soup.find('div', {'class': 'product__price'})\n",
    "        if item_price is not None:\n",
    "            item_price = item_price.find_all('span')\n",
    "            item_price = [item_price.text for item_price in item_price]\n",
    "            item_price = item_price[0]\n",
    "            #strip $ fro the price\n",
    "            item_price = item_price[1:]\n",
    "        else:\n",
    "            item_price = \"\"\n",
    "\n",
    "\n",
    "        # Find the page description element\n",
    "        item_description = soup.find('div', {'class': 'product__accordion'})\n",
    "        if item_description is not None:\n",
    "\n",
    "            # Product Description\n",
    "            product_description = item_description.find('div', {'id': 'accordion-panel-1'})\n",
    "            if product_description is not None:\n",
    "                product_description = product_description.text.strip()\n",
    "            else: \n",
    "                product_description = \"\"\n",
    "\n",
    "            # Product Details\n",
    "            product_details = item_description.find('div', {'id': 'accordion-panel-2'})\n",
    "            if product_details is not None:\n",
    "                # from the unordered list, get the text of each list item\n",
    "                product_details = product_details.find_all('li')\n",
    "                product_details = [li.text for li in product_details]\n",
    "                product_details = \"\\n\".join(product_details)\n",
    "            else: \n",
    "                product_details = \"\"        \n",
    "            \n",
    "        else:\n",
    "            item_description = \"\"\n",
    "    \n",
    "        # Find the item image\n",
    "        item_img = soup.find('div', {'class': 'product__media'})\n",
    "        if item_img is not None:\n",
    "            item_img = item_img.find('img', {'class': 'component-image__image'})\n",
    "            item_img = item_img['src']\n",
    "            item_img = item_img.split('?')[0]\n",
    "        else:\n",
    "            item_img = \"\"\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not get the description from the URL: {job_url}\")\n",
    "        logging.error(e)\n",
    "        exit()\n",
    "\n",
    "    item['title'] = item_title\n",
    "    item['price'] = item_price\n",
    "    item['img'] = item_img\n",
    "    item['description'] = f\"Description:\\n{clean_text(product_description)}\\n\\nDetails:\\n{product_details}\"\n",
    "\n",
    "\n",
    "    return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get list of URLs from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read urls from file named links.txt\n",
    "with open('links.txt', 'r') as file:\n",
    "    urls = file.readlines()\n",
    "    urls = [url.strip() for url in urls]\n",
    "\n",
    "# prepend \"https://blueillusion.com/\" to each url\n",
    "urls = [\"https://blueillusion.com\" + url for url in urls]\n",
    "\n",
    "# For debugging, override the list and use only a single URL\n",
    "urls = [\"https://blueillusion.com/products/waist-tab-linen-culotte-216716lnm-chambray-cross-dye\"]\n",
    "\n",
    "print (f\"Number of URLs: {len(urls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through the URLs and create a LangChain Document object for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "data = []\n",
    "for url in urls:\n",
    "    item = website_to_txt(url)\n",
    "    \n",
    "    metadata = {\n",
    "        'source': url,\n",
    "        'title': item['title'],\n",
    "        'price': item['price'],\n",
    "        'img': item['img'],\n",
    "        'language': 'en'\n",
    "    }\n",
    "\n",
    "    document = Document(page_content=item['description'], metadata=metadata)\n",
    "\n",
    "    data.append(document)\n",
    "\n",
    "print (f\"Number of Documents: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Crawled Data to Disk**\n",
    "\n",
    "*WARNING: Only run this block if you want to recreate the Pickle file*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#write data to file in a way that it can be reconstituted into a list of documents\n",
    "with open(\"website_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Crawled Data from Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read help_data.pkl and recreate data object as list of documents\n",
    "import pickle\n",
    "with open(\"website_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# Chunk the data\n",
    "print(\"Splitting Data\")\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=50)\n",
    "docs = text_splitter.split_documents(data)\n",
    "print(f\"Number of chunks: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AstraDB Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "ASTRA_DB_APPLICATION_TOKEN = os.environ[\"ASTRA_DB_APPLICATION_TOKEN\"]\n",
    "ASTRA_VECTOR_ENDPOINT = os.environ[\"ASTRA_VECTOR_ENDPOINT\"]\n",
    "ASTRA_DB_KEYSPACE = \"blueillusion\"\n",
    "ASTRA_DB_COLLECTION = \"catalogue\"\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "AWS_ACCESS_KEY_ID = os.environ[\"AWS_ACCESS_KEY_ID\"]\n",
    "AWS_SECRET_ACCESS_KEY = os.environ[\"AWS_SECRET_ACCESS_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores.astradb import AstraDB\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Set up the vector store\n",
    "print(f\"Setup Vector Store: {ASTRA_DB_KEYSPACE} - {ASTRA_DB_COLLECTION}\")\n",
    "vectorstore = AstraDB(\n",
    "    embedding=embeddings,\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    collection_name=ASTRA_DB_COLLECTION,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_VECTOR_ENDPOINT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store data and embeddings in Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"Adding texts to Vector Store\")\n",
    "\n",
    "BLOCK_SIZE = 50\n",
    "# iterate through docs in sets of BLOCK_SIZE\n",
    "for i in range(0, len(docs), BLOCK_SIZE):\n",
    "    print(f\"Adding {i} to {i+BLOCK_SIZE}\", end=' ')\n",
    "    texts, metadatas = zip(*((doc.page_content, doc.metadata) for doc in docs[i:i+BLOCK_SIZE]))\n",
    "    inserted_ids = vectorstore.add_texts(texts=texts, metadatas=metadatas)\n",
    "    print(f\"Inserted {len(inserted_ids)} documents.\")\n",
    "    # pause for 1 seconds\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete all Documents in Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*WARNING*: This code will delete all documents from the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrapy.db import AstraDB\n",
    "\n",
    "# Initialize the AstraDB client\n",
    "db = AstraDB(\n",
    "    namespace=ASTRA_DB_KEYSPACE,\n",
    "    token=ASTRA_DB_APPLICATION_TOKEN,\n",
    "    api_endpoint=ASTRA_VECTOR_ENDPOINT,\n",
    ")\n",
    "\n",
    "# Retrieve collections\n",
    "collections_response = db.get_collections()\n",
    "\n",
    "# validate that ASTRA_DB_COLLECTION exists in collections_response[\"status\"][\"collections\"]\n",
    "if ASTRA_DB_COLLECTION in collections_response[\"status\"][\"collections\"]:\n",
    "    print(f\"Collection \\\"{ASTRA_DB_COLLECTION}\\\" exists\")\n",
    "\n",
    "    # Access an existing collection\n",
    "    collection = db.collection(ASTRA_DB_COLLECTION)\n",
    "\n",
    "    # Delete all documents in the collection\n",
    "    res = collection.delete_many(filter={})\n",
    "\n",
    "    # Print the result\n",
    "    print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_vector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
